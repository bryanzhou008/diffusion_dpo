{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9870a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Parameter Setup\n",
    "\n",
    "# Sampling parameters\n",
    "num_train_per_prompt = 9\n",
    "dialects = [\"ine\", \"bre\", \"aae\", \"sge\", \"che\"]\n",
    "method = \"sft\"           # \"dpo\" or \"sft\"\n",
    "prompt_stype = \"concise\"   # \"concsie\" or \"detailed\"\n",
    "generative_model = \"stable-diffusion1.5\"\n",
    "\n",
    "# Experiment naming\n",
    "exp_name = f\"all_dialects-{method}-{prompt_stype}-4-1-1-{num_train_per_prompt}\"\n",
    "num_processes = 4\n",
    "\n",
    "# Path templates (use .format(dialect=...) in Cell 2)\n",
    "csv_template = (\n",
    "    \"/local1/bryanzhou008/Dialect/\"\n",
    "    \"multimodal-dialectal-bias/data/text/train_val_test/4-1-1/\"\n",
    "    f\"{prompt_stype}/{{dialect}}/train.csv\"\n",
    ")\n",
    "images_template = (\n",
    "    \"/local1/bryanzhou008/Dialect/\"\n",
    "    f\"multimodal-dialectal-bias/data/image/{prompt_stype}/\"\n",
    "    \"{dialect}/\" + generative_model\n",
    ")\n",
    "\n",
    "# Output directories\n",
    "output_folder = (\n",
    "    \"/local1/bryanzhou008/Dialect/\"\n",
    "    f\"multimodal-dialectal-bias/mitigation/baselines/\"\n",
    "    f\"diffusion_dpo/a_data/{exp_name}\"\n",
    ")\n",
    "\n",
    "# Training script template\n",
    "script_command = f\"\"\"#!/bin/bash\n",
    "export MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
    "export DATA_DIR=\"{output_folder}\"\n",
    "\n",
    "accelerate launch --num_processes {num_processes} \\\\\n",
    "  /local1/bryanzhou008/Dialect/multimodal-dialectal-bias/mitigation/baselines/diffusion_dpo/a_src/train/train.py \\\\\n",
    "    --pretrained_model_name_or_path=$MODEL_NAME \\\\\n",
    "    --train_data_dir=$DATA_DIR \\\\\n",
    "    --train_batch_size=1 \\\\\n",
    "    --dataloader_num_workers=16 \\\\\n",
    "    --gradient_accumulation_steps=128 \\\\\n",
    "    --max_train_steps=2000 \\\\\n",
    "    --lr_scheduler=\"constant_with_warmup\" --lr_warmup_steps=500 \\\\\n",
    "    --learning_rate=1e-8 --scale_lr \\\\\n",
    "    --cache_dir=\"/local1/bryanzhou008/Dialect/multimodal-dialectal-bias/mitigation/baselines/diffusion_dpo/temp_cache/\" \\\\\n",
    "    --checkpointing_steps=200 \\\\\n",
    "    --beta_dpo=5000 \\\\\n",
    "    --output_dir=\"/local1/bryanzhou008/Dialect/multimodal-dialectal-bias/mitigation/baselines/diffusion_dpo/a_checkpoints/{exp_name}\" \\\\\n",
    "    --report_to \"wandb\" \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51566e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Preparation and Script Generation\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Create unified train directory\n",
    "train_dir = os.path.join(output_folder, \"train\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "metadata_entries = []\n",
    "global_counter = 0\n",
    "\n",
    "# Loop over each dialect\n",
    "for dialect in dialects:\n",
    "    # Resolve paths for this dialect\n",
    "    csv_file = csv_template.format(dialect=dialect)\n",
    "    input_images_folder = images_template.format(dialect=dialect)\n",
    "    \n",
    "    # Read CSV\n",
    "    with open(csv_file, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row_index, row in enumerate(reader):\n",
    "            dialect_prompt = row[\"Dialect_Prompt\"].strip()\n",
    "            sae_prompt     = row[\"SAE_Prompt\"].strip()\n",
    "            prompt_text    = dialect_prompt  # caption\n",
    "\n",
    "            # Source subfolders\n",
    "            lose_dir = os.path.join(input_images_folder, \"dialect_imgs\", dialect_prompt)\n",
    "            win_dir  = os.path.join(input_images_folder, \"sae_imgs\",     sae_prompt)\n",
    "\n",
    "            # Sample first N images\n",
    "            for i in range(num_train_per_prompt):\n",
    "                win_src = os.path.join(win_dir,  f\"{i}.jpg\")\n",
    "                if method == \"dpo\":\n",
    "                    lose_src = os.path.join(lose_dir, f\"{i}.jpg\")\n",
    "                \n",
    "                # Skip if missing\n",
    "                if not os.path.exists(win_src):\n",
    "                    print(f\"Warning: missing win {win_src}\")\n",
    "                    continue\n",
    "                if method == \"dpo\" and not os.path.exists(lose_src):\n",
    "                    print(f\"Warning: missing lose {lose_src}\")\n",
    "                    continue\n",
    "\n",
    "                # Destination filenames\n",
    "                win_name  = f\"win_{global_counter}.jpg\"\n",
    "                win_dest  = os.path.join(train_dir, win_name)\n",
    "                shutil.copy(win_src, win_dest)\n",
    "\n",
    "                if method == \"dpo\":\n",
    "                    lose_name = f\"lose_{global_counter}.jpg\"\n",
    "                    lose_dest = os.path.join(train_dir, lose_name)\n",
    "                    shutil.copy(lose_src, lose_dest)\n",
    "\n",
    "                # Build metadata entries\n",
    "                if method == \"dpo\":\n",
    "                    metadata_entries.append({\n",
    "                        \"file_name\": win_name,\n",
    "                        \"jpg_0\":     win_name,\n",
    "                        \"jpg_1\":     lose_name,\n",
    "                        \"label_0\":   1,\n",
    "                        \"caption\":   prompt_text\n",
    "                    })\n",
    "                    metadata_entries.append({\n",
    "                        \"file_name\": lose_name,\n",
    "                        \"jpg_0\":     win_name,\n",
    "                        \"jpg_1\":     lose_name,\n",
    "                        \"label_0\":   1,\n",
    "                        \"caption\":   prompt_text\n",
    "                    })\n",
    "                else:  # sft\n",
    "                    metadata_entries.append({\n",
    "                        \"file_name\": win_name,\n",
    "                        \"jpg_0\":     win_name,\n",
    "                        \"jpg_1\":     win_name,\n",
    "                        \"label_0\":   1,\n",
    "                        \"caption\":   prompt_text\n",
    "                    })\n",
    "\n",
    "                global_counter += 1\n",
    "\n",
    "# Write metadata.jsonl\n",
    "metadata_path = os.path.join(train_dir, \"metadata.jsonl\")\n",
    "with open(metadata_path, \"w\", encoding='utf-8') as f:\n",
    "    for entry in metadata_entries:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"Dataset created at {output_folder}\")\n",
    "print(f\"Total metadata entries: {len(metadata_entries)}\")\n",
    "\n",
    "# Write training script\n",
    "script_path = os.path.join(output_folder, \"run_training.sh\")\n",
    "with open(script_path, \"w\", encoding='utf-8') as f:\n",
    "    f.write(script_command)\n",
    "os.chmod(script_path, 0o755)\n",
    "\n",
    "print(f\"Training script created at {script_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
